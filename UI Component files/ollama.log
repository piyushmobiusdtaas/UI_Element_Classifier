Couldn't find '/root/.ollama/id_ed25519'. Generating new private key.
Your new public key is: 

ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAII+9jRu70sYAAFWkos5yM/iP0lF3z3PPkTEFYZeTESmk

time=2025-06-27T12:25:22.531Z level=INFO source=routes.go:1235 msg="server config" env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/root/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-06-27T12:25:22.531Z level=INFO source=images.go:476 msg="total blobs: 0"
time=2025-06-27T12:25:22.531Z level=INFO source=images.go:483 msg="total unused blobs removed: 0"
time=2025-06-27T12:25:22.531Z level=INFO source=routes.go:1288 msg="Listening on 127.0.0.1:11434 (version 0.9.3)"
time=2025-06-27T12:25:22.532Z level=INFO source=gpu.go:217 msg="looking for compatible GPUs"
time=2025-06-27T12:25:22.747Z level=INFO source=types.go:130 msg="inference compute" id=GPU-426168b1-af4c-a3aa-d9ec-46c13501e940 library=cuda variant=v12 compute=8.0 driver=12.4 name="NVIDIA A100-SXM4-40GB" total="39.6 GiB" available="39.1 GiB"
[GIN] 2025/06/27 - 12:25:25 | 200 |      74.741µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/27 - 12:25:25 | 404 |     382.172µs |       127.0.0.1 | POST     "/api/show"
time=2025-06-27T12:25:25.629Z level=INFO source=download.go:177 msg="downloading 043a363c6ca3 in 22 1 GB part(s)"
time=2025-06-27T12:26:29.788Z level=INFO source=download.go:177 msg="downloading a242d8dfdc8f in 1 487 B part(s)"
time=2025-06-27T12:26:30.931Z level=INFO source=download.go:177 msg="downloading 75357d685f23 in 1 28 B part(s)"
time=2025-06-27T12:26:32.079Z level=INFO source=download.go:177 msg="downloading 832dd9e00a68 in 1 11 KB part(s)"
time=2025-06-27T12:26:33.298Z level=INFO source=download.go:177 msg="downloading 52d2a7aa3a38 in 1 23 B part(s)"
time=2025-06-27T12:26:34.492Z level=INFO source=download.go:177 msg="downloading 61b48efc2682 in 1 568 B part(s)"
[GIN] 2025/06/27 - 12:27:43 | 200 |         2m17s |       127.0.0.1 | POST     "/api/pull"
[GIN] 2025/06/27 - 12:27:43 | 200 |   81.001699ms |       127.0.0.1 | POST     "/api/show"
time=2025-06-27T12:27:43.361Z level=INFO source=sched.go:788 msg="new model will fit in available VRAM in single GPU, loading" model=/root/.ollama/models/blobs/sha256-043a363c6ca35e3b1a29b8a5b0bbd28474820239bbc5ad943c9be18f0dc77b66 gpu=GPU-426168b1-af4c-a3aa-d9ec-46c13501e940 parallel=2 available=42029940736 required="25.3 GiB"
time=2025-06-27T12:27:43.474Z level=INFO source=server.go:135 msg="system memory" total="83.5 GiB" free="81.3 GiB" free_swap="0 B"
time=2025-06-27T12:27:43.477Z level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=65 layers.offload=65 layers.split="" memory.available="[39.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="25.3 GiB" memory.required.partial="25.3 GiB" memory.required.kv="2.0 GiB" memory.required.allocations="[25.3 GiB]" memory.weights.total="18.1 GiB" memory.weights.repeating="17.5 GiB" memory.weights.nonrepeating="609.1 MiB" memory.graph.full="1.7 GiB" memory.graph.partial="1.7 GiB" projector.weights="1.2 GiB" projector.graph="1.6 GiB"
time=2025-06-27T12:27:43.520Z level=INFO source=server.go:438 msg="starting llama server" cmd="/usr/local/bin/ollama runner --ollama-engine --model /root/.ollama/models/blobs/sha256-043a363c6ca35e3b1a29b8a5b0bbd28474820239bbc5ad943c9be18f0dc77b66 --ctx-size 8192 --batch-size 512 --n-gpu-layers 65 --threads 6 --parallel 2 --port 40799"
time=2025-06-27T12:27:43.520Z level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-06-27T12:27:43.520Z level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-06-27T12:27:43.520Z level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-06-27T12:27:43.534Z level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-06-27T12:27:43.537Z level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:40799"
time=2025-06-27T12:27:43.581Z level=INFO source=ggml.go:92 msg="" architecture=qwen25vl file_type=Q4_K_M name="" description="" num_tensors=1290 num_key_values=36
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /usr/local/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-skylakex.so
time=2025-06-27T12:27:43.654Z level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-06-27T12:27:43.750Z level=INFO source=ggml.go:359 msg="model weights" buffer=CUDA0 size="19.3 GiB"
time=2025-06-27T12:27:43.750Z level=INFO source=ggml.go:359 msg="model weights" buffer=CPU size="417.7 MiB"
time=2025-06-27T12:27:43.771Z level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-06-27T12:27:44.236Z level=INFO source=ggml.go:648 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="1.7 GiB"
time=2025-06-27T12:27:44.236Z level=INFO source=ggml.go:648 msg="compute graph" backend=CPU buffer_type=CPU size="0 B"
time=2025-06-27T12:27:44.257Z level=INFO source=ggml.go:648 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="1.7 GiB"
time=2025-06-27T12:27:44.257Z level=INFO source=ggml.go:648 msg="compute graph" backend=CPU buffer_type=CPU size="23.9 MiB"
time=2025-06-27T12:27:51.549Z level=INFO source=server.go:637 msg="llama runner started in 8.03 seconds"
[GIN] 2025/06/27 - 12:27:54 | 200 | 11.013460658s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/06/27 - 12:28:06 | 200 |      27.039µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/06/27 - 12:28:06 | 200 |     335.318µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/06/27 - 12:29:35 | 200 |  27.83635666s |       127.0.0.1 | POST     "/api/chat"
